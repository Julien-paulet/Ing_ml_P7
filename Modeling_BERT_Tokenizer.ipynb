{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from transformers import TFBertModel,  BertConfig, BertTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "import pickle\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the randomness\n",
    "from numpy.random import seed\n",
    "seed(1337)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalStrategy = \"retrain\" # Either retrain or keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init function to print f1 score and Jaccard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(data, predict):\n",
    "    f1 = []\n",
    "    jaccard = []\n",
    "    threshold = []\n",
    "    for i in np.arange(0.30, 0.99, 0.01):\n",
    "        predict_ = np.where(predict >= i, 1, 0)\n",
    "        f1_micro = metrics.f1_score(data, predict_, average = 'micro')\n",
    "        jaccard_micro = metrics.jaccard_score(data, predict_, average = 'micro')\n",
    "        f1.append(f1_micro)\n",
    "        jaccard.append(jaccard_micro)\n",
    "        threshold.append(i)\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    results['Threshold'] = threshold\n",
    "    results['F1_micro'] = f1\n",
    "    results['Jaccard_micro'] = jaccard\n",
    "    results = results[results['F1_micro'] == results['F1_micro'].max()]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining F1 Metric by hand as Keras does not support it naturally now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>unstemmed_desc</th>\n",
       "      <th>preprocessedTags</th>\n",
       "      <th>Tag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vs cakephp vs zend vs cakephp vs zend cakephp ...</td>\n",
       "      <td>vs cakephp vs zend vs cakephp vs zend cakephp ...</td>\n",
       "      <td>[php]</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tool generat mock data tool generat mock data ...</td>\n",
       "      <td>tools generating mock data tools generating mo...</td>\n",
       "      <td>[testing]</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>laravel use statement non name cach effect lar...</td>\n",
       "      <td>laravel use statement non name cache effect la...</td>\n",
       "      <td>[php, laravel]</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>add client authent add client authent server r...</td>\n",
       "      <td>add client authentication add client authentic...</td>\n",
       "      <td>[java]</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>variabl error variabl error system namespac cl...</td>\n",
       "      <td>variable error variable error system namespace...</td>\n",
       "      <td>[c#]</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                desc  \\\n",
       "0  vs cakephp vs zend vs cakephp vs zend cakephp ...   \n",
       "1  tool generat mock data tool generat mock data ...   \n",
       "2  laravel use statement non name cach effect lar...   \n",
       "3  add client authent add client authent server r...   \n",
       "4  variabl error variabl error system namespac cl...   \n",
       "\n",
       "                                      unstemmed_desc preprocessedTags     Tag1  \n",
       "0  vs cakephp vs zend vs cakephp vs zend cakephp ...            [php]      php  \n",
       "1  tools generating mock data tools generating mo...        [testing]  testing  \n",
       "2  laravel use statement non name cache effect la...   [php, laravel]      php  \n",
       "3  add client authentication add client authentic...           [java]     java  \n",
       "4  variable error variable error system namespace...             [c#]       c#  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_cleaned.csv',\n",
    "                   converters={\"preprocessedTags\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "#data = pd.read_csv('data_cleaned2.csv',\n",
    "#                   converters={\"preprocessedTags\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "\n",
    "data = data[['desc', 'unstemmed_desc', 'preprocessedTags', 'Tag1']].dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide dataset into X and Y values, and transform Y to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['desc']\n",
    "x_unstemmed = data['unstemmed_desc']\n",
    "y = data['preprocessedTags']\n",
    "# y_tag = data['Tag1']\n",
    "mb = MultiLabelBinarizer()\n",
    "y_encoded = mb.fit_transform(y)\n",
    "# y_encoded = mb.fit_transform(y_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 750\n",
    "tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
    "tfidf.fit(x)\n",
    "tfidf.transform(x)\n",
    "\n",
    "tfidf_vector_X = tfidf.transform(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_vector_X, \n",
    "                                                                            y_encoded, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xInput = Input(shape=(x_train_tfidf.shape[1:]))\n",
    "x_ = Dense(700, activation='relu')(xInput)\n",
    "x_ = Dense(700, activation='relu')(x_)\n",
    "output = Dense(len(y_train_tfidf[1]), activation='sigmoid')(x_)\n",
    "\n",
    "baseline = Model(inputs=xInput, outputs=output, name='Baseline')\n",
    "\n",
    "# Compile the model\n",
    "baseline.compile(loss=CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "                 optimizer=Adam(learning_rate=0.0000001),\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=10,\n",
    "                                            mode='max',\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Baseline\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 750)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 700)               525700    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 700)               490700    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 206)               144406    \n",
      "=================================================================\n",
      "Total params: 1,160,806\n",
      "Trainable params: 1,160,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.6139 - acc: 0.0062 - val_loss: 9.7354 - val_acc: 0.0050\n",
      "Epoch 2/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5843 - acc: 0.0082 - val_loss: 9.7322 - val_acc: 0.0074\n",
      "Epoch 3/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5100 - acc: 0.0102 - val_loss: 9.7291 - val_acc: 0.0093\n",
      "Epoch 4/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5734 - acc: 0.0126 - val_loss: 9.7260 - val_acc: 0.0122\n",
      "Epoch 5/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5252 - acc: 0.0146 - val_loss: 9.7228 - val_acc: 0.0130\n",
      "Epoch 6/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4963 - acc: 0.0186 - val_loss: 9.7196 - val_acc: 0.0194\n",
      "Epoch 7/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5514 - acc: 0.0215 - val_loss: 9.7164 - val_acc: 0.0252\n",
      "Epoch 8/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5161 - acc: 0.0279 - val_loss: 9.7132 - val_acc: 0.0284\n",
      "Epoch 9/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5320 - acc: 0.0308 - val_loss: 9.7100 - val_acc: 0.0297\n",
      "Epoch 10/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5526 - acc: 0.0374 - val_loss: 9.7067 - val_acc: 0.0361\n",
      "Epoch 11/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5042 - acc: 0.0412 - val_loss: 9.7035 - val_acc: 0.0401\n",
      "Epoch 12/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5323 - acc: 0.0467 - val_loss: 9.7001 - val_acc: 0.0433\n",
      "Epoch 13/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5173 - acc: 0.0499 - val_loss: 9.6968 - val_acc: 0.0473\n",
      "Epoch 14/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5219 - acc: 0.0544 - val_loss: 9.6934 - val_acc: 0.0510\n",
      "Epoch 15/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5041 - acc: 0.0557 - val_loss: 9.6899 - val_acc: 0.0518\n",
      "Epoch 16/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5408 - acc: 0.0595 - val_loss: 9.6864 - val_acc: 0.0539\n",
      "Epoch 17/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5356 - acc: 0.0589 - val_loss: 9.6829 - val_acc: 0.0558\n",
      "Epoch 18/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4972 - acc: 0.0617 - val_loss: 9.6793 - val_acc: 0.0563\n",
      "Epoch 19/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5101 - acc: 0.0653 - val_loss: 9.6757 - val_acc: 0.0574\n",
      "Epoch 20/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5154 - acc: 0.0651 - val_loss: 9.6720 - val_acc: 0.0563\n",
      "Epoch 21/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4837 - acc: 0.0684 - val_loss: 9.6682 - val_acc: 0.0566\n",
      "Epoch 22/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4710 - acc: 0.0704 - val_loss: 9.6644 - val_acc: 0.0571\n",
      "Epoch 23/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4841 - acc: 0.0666 - val_loss: 9.6605 - val_acc: 0.0566\n",
      "Epoch 24/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4793 - acc: 0.0709 - val_loss: 9.6566 - val_acc: 0.0582\n",
      "Epoch 25/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5232 - acc: 0.0681 - val_loss: 9.6525 - val_acc: 0.0595\n",
      "Epoch 26/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.5130 - acc: 0.0673 - val_loss: 9.6485 - val_acc: 0.0600\n",
      "Epoch 27/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4840 - acc: 0.0686 - val_loss: 9.6443 - val_acc: 0.0600\n",
      "Epoch 28/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4361 - acc: 0.0690 - val_loss: 9.6401 - val_acc: 0.0606\n",
      "Epoch 29/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4427 - acc: 0.0698 - val_loss: 9.6358 - val_acc: 0.0606\n",
      "Epoch 30/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4619 - acc: 0.0710 - val_loss: 9.6314 - val_acc: 0.0606\n",
      "Epoch 31/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4556 - acc: 0.0716 - val_loss: 9.6269 - val_acc: 0.0622\n",
      "Epoch 32/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4749 - acc: 0.0683 - val_loss: 9.6224 - val_acc: 0.0624\n",
      "Epoch 33/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4408 - acc: 0.0701 - val_loss: 9.6177 - val_acc: 0.0632\n",
      "Epoch 34/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4076 - acc: 0.0712 - val_loss: 9.6130 - val_acc: 0.0648\n",
      "Epoch 35/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4013 - acc: 0.0740 - val_loss: 9.6082 - val_acc: 0.0651\n",
      "Epoch 36/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4396 - acc: 0.0713 - val_loss: 9.6033 - val_acc: 0.0661\n",
      "Epoch 37/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4482 - acc: 0.0737 - val_loss: 9.5983 - val_acc: 0.0669\n",
      "Epoch 38/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4528 - acc: 0.0758 - val_loss: 9.5932 - val_acc: 0.0680\n",
      "Epoch 39/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4089 - acc: 0.0737 - val_loss: 9.5880 - val_acc: 0.0685\n",
      "Epoch 40/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3982 - acc: 0.0784 - val_loss: 9.5827 - val_acc: 0.0693\n",
      "Epoch 41/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3895 - acc: 0.0758 - val_loss: 9.5773 - val_acc: 0.0696\n",
      "Epoch 42/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.4106 - acc: 0.0764 - val_loss: 9.5718 - val_acc: 0.0712\n",
      "Epoch 43/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3847 - acc: 0.0793 - val_loss: 9.5662 - val_acc: 0.0720\n",
      "Epoch 44/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3851 - acc: 0.0786 - val_loss: 9.5605 - val_acc: 0.0728\n",
      "Epoch 45/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3662 - acc: 0.0786 - val_loss: 9.5547 - val_acc: 0.0730\n",
      "Epoch 46/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3635 - acc: 0.0813 - val_loss: 9.5487 - val_acc: 0.0738\n",
      "Epoch 47/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3395 - acc: 0.0791 - val_loss: 9.5427 - val_acc: 0.0744\n",
      "Epoch 48/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3889 - acc: 0.0802 - val_loss: 9.5365 - val_acc: 0.0752\n",
      "Epoch 49/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3353 - acc: 0.0811 - val_loss: 9.5303 - val_acc: 0.0754\n",
      "Epoch 50/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3170 - acc: 0.0842 - val_loss: 9.5239 - val_acc: 0.0762\n",
      "Epoch 51/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3798 - acc: 0.0835 - val_loss: 9.5173 - val_acc: 0.0765\n",
      "Epoch 52/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3548 - acc: 0.0823 - val_loss: 9.5107 - val_acc: 0.0770\n",
      "Epoch 53/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3315 - acc: 0.0863 - val_loss: 9.5039 - val_acc: 0.0778\n",
      "Epoch 54/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3401 - acc: 0.0837 - val_loss: 9.4970 - val_acc: 0.0781\n",
      "Epoch 55/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3103 - acc: 0.0865 - val_loss: 9.4900 - val_acc: 0.0786\n",
      "Epoch 56/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2916 - acc: 0.0847 - val_loss: 9.4829 - val_acc: 0.0786\n",
      "Epoch 57/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2955 - acc: 0.0875 - val_loss: 9.4756 - val_acc: 0.0792\n",
      "Epoch 58/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3035 - acc: 0.0868 - val_loss: 9.4682 - val_acc: 0.0797\n",
      "Epoch 59/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2680 - acc: 0.0900 - val_loss: 9.4606 - val_acc: 0.0789\n",
      "Epoch 60/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.3076 - acc: 0.0882 - val_loss: 9.4529 - val_acc: 0.0794\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2697 - acc: 0.0918 - val_loss: 9.4451 - val_acc: 0.0802\n",
      "Epoch 62/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2336 - acc: 0.0915 - val_loss: 9.4371 - val_acc: 0.0805\n",
      "Epoch 63/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2653 - acc: 0.0891 - val_loss: 9.4290 - val_acc: 0.0813\n",
      "Epoch 64/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2714 - acc: 0.0924 - val_loss: 9.4207 - val_acc: 0.0821\n",
      "Epoch 65/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2646 - acc: 0.0924 - val_loss: 9.4123 - val_acc: 0.0831\n",
      "Epoch 66/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2605 - acc: 0.0910 - val_loss: 9.4037 - val_acc: 0.0834\n",
      "Epoch 67/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2377 - acc: 0.0924 - val_loss: 9.3950 - val_acc: 0.0839\n",
      "Epoch 68/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2160 - acc: 0.0936 - val_loss: 9.3861 - val_acc: 0.0842\n",
      "Epoch 69/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1899 - acc: 0.0954 - val_loss: 9.3771 - val_acc: 0.0850\n",
      "Epoch 70/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2292 - acc: 0.0946 - val_loss: 9.3679 - val_acc: 0.0858\n",
      "Epoch 71/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.2016 - acc: 0.0957 - val_loss: 9.3586 - val_acc: 0.0869\n",
      "Epoch 72/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1351 - acc: 0.0973 - val_loss: 9.3491 - val_acc: 0.0871\n",
      "Epoch 73/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1473 - acc: 0.0974 - val_loss: 9.3395 - val_acc: 0.0882\n",
      "Epoch 74/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1918 - acc: 0.0979 - val_loss: 9.3296 - val_acc: 0.0882\n",
      "Epoch 75/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1420 - acc: 0.0977 - val_loss: 9.3197 - val_acc: 0.0887\n",
      "Epoch 76/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1286 - acc: 0.0975 - val_loss: 9.3095 - val_acc: 0.0892\n",
      "Epoch 77/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.0971 - acc: 0.1018 - val_loss: 9.2992 - val_acc: 0.0895\n",
      "Epoch 78/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1690 - acc: 0.1002 - val_loss: 9.2888 - val_acc: 0.0895\n",
      "Epoch 79/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1035 - acc: 0.0998 - val_loss: 9.2781 - val_acc: 0.0906\n",
      "Epoch 80/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1081 - acc: 0.1028 - val_loss: 9.2674 - val_acc: 0.0908\n",
      "Epoch 81/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.1069 - acc: 0.1036 - val_loss: 9.2564 - val_acc: 0.0914\n",
      "Epoch 82/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.0793 - acc: 0.1007 - val_loss: 9.2453 - val_acc: 0.0927\n",
      "Epoch 83/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.0782 - acc: 0.1043 - val_loss: 9.2340 - val_acc: 0.0935\n",
      "Epoch 84/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.0827 - acc: 0.1082 - val_loss: 9.2225 - val_acc: 0.0938\n",
      "Epoch 85/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.0669 - acc: 0.1025 - val_loss: 9.2109 - val_acc: 0.0943\n",
      "Epoch 86/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.0535 - acc: 0.1074 - val_loss: 9.1991 - val_acc: 0.0946\n",
      "Epoch 87/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9888 - acc: 0.1067 - val_loss: 9.1872 - val_acc: 0.0961\n",
      "Epoch 88/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9.0349 - acc: 0.1062 - val_loss: 9.1751 - val_acc: 0.0961\n",
      "Epoch 89/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9777 - acc: 0.1080 - val_loss: 9.1628 - val_acc: 0.0975\n",
      "Epoch 90/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9845 - acc: 0.1113 - val_loss: 9.1504 - val_acc: 0.0980\n",
      "Epoch 91/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9558 - acc: 0.1095 - val_loss: 9.1378 - val_acc: 0.0985\n",
      "Epoch 92/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9930 - acc: 0.1071 - val_loss: 9.1250 - val_acc: 0.0991\n",
      "Epoch 93/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9370 - acc: 0.1090 - val_loss: 9.1121 - val_acc: 0.0996\n",
      "Epoch 94/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9613 - acc: 0.1095 - val_loss: 9.0990 - val_acc: 0.1012\n",
      "Epoch 95/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9592 - acc: 0.1133 - val_loss: 9.0857 - val_acc: 0.1015\n",
      "Epoch 96/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.9062 - acc: 0.1155 - val_loss: 9.0723 - val_acc: 0.1017\n",
      "Epoch 97/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8944 - acc: 0.1143 - val_loss: 9.0588 - val_acc: 0.1025\n",
      "Epoch 98/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8380 - acc: 0.1174 - val_loss: 9.0451 - val_acc: 0.1036\n",
      "Epoch 99/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8428 - acc: 0.1119 - val_loss: 9.0312 - val_acc: 0.1031\n",
      "Epoch 100/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8603 - acc: 0.1127 - val_loss: 9.0172 - val_acc: 0.1041\n",
      "Epoch 101/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8169 - acc: 0.1171 - val_loss: 9.0031 - val_acc: 0.1054\n",
      "Epoch 102/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8331 - acc: 0.1179 - val_loss: 8.9888 - val_acc: 0.1057\n",
      "Epoch 103/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8536 - acc: 0.1184 - val_loss: 8.9744 - val_acc: 0.1060\n",
      "Epoch 104/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.8385 - acc: 0.1167 - val_loss: 8.9599 - val_acc: 0.1070\n",
      "Epoch 105/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.7954 - acc: 0.1181 - val_loss: 8.9452 - val_acc: 0.1076\n",
      "Epoch 106/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.7740 - acc: 0.1213 - val_loss: 8.9304 - val_acc: 0.1078\n",
      "Epoch 107/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.7849 - acc: 0.1201 - val_loss: 8.9155 - val_acc: 0.1084\n",
      "Epoch 108/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.7481 - acc: 0.1204 - val_loss: 8.9004 - val_acc: 0.1092\n",
      "Epoch 109/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.7010 - acc: 0.1226 - val_loss: 8.8853 - val_acc: 0.1097\n",
      "Epoch 110/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.6946 - acc: 0.1223 - val_loss: 8.8701 - val_acc: 0.1108\n",
      "Epoch 111/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.6798 - acc: 0.1280 - val_loss: 8.8548 - val_acc: 0.1121\n",
      "Epoch 112/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.6868 - acc: 0.1226 - val_loss: 8.8394 - val_acc: 0.1126\n",
      "Epoch 113/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.6584 - acc: 0.1233 - val_loss: 8.8239 - val_acc: 0.1126\n",
      "Epoch 114/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.6863 - acc: 0.1253 - val_loss: 8.8084 - val_acc: 0.1126\n",
      "Epoch 115/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.6222 - acc: 0.1266 - val_loss: 8.7927 - val_acc: 0.1142\n",
      "Epoch 116/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.6444 - acc: 0.1268 - val_loss: 8.7770 - val_acc: 0.1158\n",
      "Epoch 117/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.5979 - acc: 0.1301 - val_loss: 8.7613 - val_acc: 0.1169\n",
      "Epoch 118/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.5963 - acc: 0.1291 - val_loss: 8.7455 - val_acc: 0.1177\n",
      "Epoch 119/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.5651 - acc: 0.1248 - val_loss: 8.7297 - val_acc: 0.1187\n",
      "Epoch 120/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.5564 - acc: 0.1302 - val_loss: 8.7139 - val_acc: 0.1187\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.5611 - acc: 0.1311 - val_loss: 8.6981 - val_acc: 0.1198\n",
      "Epoch 122/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.5121 - acc: 0.1330 - val_loss: 8.6823 - val_acc: 0.1214\n",
      "Epoch 123/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.4959 - acc: 0.1310 - val_loss: 8.6665 - val_acc: 0.1222\n",
      "Epoch 124/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.5064 - acc: 0.1330 - val_loss: 8.6507 - val_acc: 0.1232\n",
      "Epoch 125/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.4843 - acc: 0.1343 - val_loss: 8.6349 - val_acc: 0.1243\n",
      "Epoch 126/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.4653 - acc: 0.1359 - val_loss: 8.6193 - val_acc: 0.1256\n",
      "Epoch 127/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.4584 - acc: 0.1409 - val_loss: 8.6036 - val_acc: 0.1275\n",
      "Epoch 128/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.4263 - acc: 0.1329 - val_loss: 8.5881 - val_acc: 0.1280\n",
      "Epoch 129/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.4325 - acc: 0.1353 - val_loss: 8.5726 - val_acc: 0.1280\n",
      "Epoch 130/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3954 - acc: 0.1378 - val_loss: 8.5573 - val_acc: 0.1286\n",
      "Epoch 131/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.4050 - acc: 0.1379 - val_loss: 8.5421 - val_acc: 0.1296\n",
      "Epoch 132/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3440 - acc: 0.1431 - val_loss: 8.5270 - val_acc: 0.1304\n",
      "Epoch 133/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3833 - acc: 0.1406 - val_loss: 8.5121 - val_acc: 0.1299\n",
      "Epoch 134/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3300 - acc: 0.1368 - val_loss: 8.4973 - val_acc: 0.1315\n",
      "Epoch 135/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3630 - acc: 0.1424 - val_loss: 8.4827 - val_acc: 0.1315\n",
      "Epoch 136/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3533 - acc: 0.1441 - val_loss: 8.4683 - val_acc: 0.1317\n",
      "Epoch 137/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.2735 - acc: 0.1457 - val_loss: 8.4542 - val_acc: 0.1315\n",
      "Epoch 138/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3021 - acc: 0.1464 - val_loss: 8.4402 - val_acc: 0.1328\n",
      "Epoch 139/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.3003 - acc: 0.1449 - val_loss: 8.4266 - val_acc: 0.1325\n",
      "Epoch 140/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.2388 - acc: 0.1471 - val_loss: 8.4131 - val_acc: 0.1333\n",
      "Epoch 141/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.2442 - acc: 0.1452 - val_loss: 8.4000 - val_acc: 0.1333\n",
      "Epoch 142/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.2065 - acc: 0.1463 - val_loss: 8.3872 - val_acc: 0.1336\n",
      "Epoch 143/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.2012 - acc: 0.1502 - val_loss: 8.3747 - val_acc: 0.1341\n",
      "Epoch 144/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1932 - acc: 0.1541 - val_loss: 8.3625 - val_acc: 0.1363\n",
      "Epoch 145/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1829 - acc: 0.1516 - val_loss: 8.3507 - val_acc: 0.1368\n",
      "Epoch 146/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1813 - acc: 0.1506 - val_loss: 8.3392 - val_acc: 0.1371\n",
      "Epoch 147/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1600 - acc: 0.1491 - val_loss: 8.3282 - val_acc: 0.1371\n",
      "Epoch 148/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1583 - acc: 0.1509 - val_loss: 8.3175 - val_acc: 0.1389\n",
      "Epoch 149/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1546 - acc: 0.1498 - val_loss: 8.3072 - val_acc: 0.1386\n",
      "Epoch 150/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1300 - acc: 0.1510 - val_loss: 8.2974 - val_acc: 0.1394\n",
      "Epoch 151/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1031 - acc: 0.1521 - val_loss: 8.2880 - val_acc: 0.1402\n",
      "Epoch 152/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1225 - acc: 0.1514 - val_loss: 8.2791 - val_acc: 0.1394\n",
      "Epoch 153/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1048 - acc: 0.1540 - val_loss: 8.2706 - val_acc: 0.1408\n",
      "Epoch 154/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1015 - acc: 0.1541 - val_loss: 8.2626 - val_acc: 0.1408\n",
      "Epoch 155/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1001 - acc: 0.1509 - val_loss: 8.2550 - val_acc: 0.1405\n",
      "Epoch 156/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.1014 - acc: 0.1510 - val_loss: 8.2480 - val_acc: 0.1405\n",
      "Epoch 157/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.0941 - acc: 0.1513 - val_loss: 8.2414 - val_acc: 0.1402\n",
      "Epoch 158/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.0842 - acc: 0.1510 - val_loss: 8.2354 - val_acc: 0.1400\n",
      "Epoch 159/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.0807 - acc: 0.1519 - val_loss: 8.2299 - val_acc: 0.1392\n",
      "Epoch 160/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.0681 - acc: 0.1499 - val_loss: 8.2249 - val_acc: 0.1384\n",
      "Epoch 161/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.0801 - acc: 0.1467 - val_loss: 8.2203 - val_acc: 0.1378\n",
      "Epoch 162/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.0280 - acc: 0.1501 - val_loss: 8.2164 - val_acc: 0.1363\n",
      "Epoch 163/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8.0489 - acc: 0.1479 - val_loss: 8.2129 - val_acc: 0.1352\n",
      "INFO:tensorflow:Assets written to: /home/mlmaster/Code/Ing_ml_P7/Baseline/assets\n"
     ]
    }
   ],
   "source": [
    "# Load the baseline, if does not exist then train one\n",
    "if globalStrategy == 'retrain' or globalStrategy == 'retrainBaseline':\n",
    "    epochs = epochs\n",
    "    batch_size=batch_size\n",
    "    history = baseline.fit(x_train_tfidf, y_train_tfidf,\n",
    "                           epochs=epochs,\n",
    "                           validation_split=0.1,\n",
    "                           callbacks=[callback],\n",
    "                           verbose=1)\n",
    "\n",
    "    baseline.save('/home/mlmaster/Code/Ing_ml_P7/Baseline/')\n",
    "    baseline = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/Baseline/')\n",
    "else:\n",
    "    try:\n",
    "        baseline = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/Baseline/')\n",
    "    except OSError:\n",
    "        epochs = epochs\n",
    "        batch_size=batch_size\n",
    "        history = baseline.fit(x_train_tfidf, y_train_tfidf,\n",
    "                               epochs=epochs,\n",
    "                               validation_split=0.1,\n",
    "                               callbacks=[callback],\n",
    "                               verbose=1)\n",
    "\n",
    "        baseline.save('/home/mlmaster/Code/Ing_ml_P7/Baseline/')\n",
    "        baseline = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/Baseline/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 0s 1ms/step - loss: 8.1414 - acc: 0.1479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.141371726989746, 0.14791201055049896]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.evaluate(x_test_tfidf, y_test_tfidf, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = baseline.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>Jaccard_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.167133</td>\n",
       "      <td>0.091187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  F1_micro  Jaccard_micro\n",
       "62       0.92  0.167133       0.091187"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict_ = np.where(predict>0.55, 1, 0)\n",
    "scoring(y_test_tfidf, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7831337 , 0.4360375 , 0.52132756, 0.73985654, 0.9072598 ,\n",
       "       0.34264836, 0.29580933, 0.52294415, 0.63381034, 0.48198608,\n",
       "       0.53523386, 0.39627838, 0.50225943, 0.84018433, 0.6914893 ,\n",
       "       0.5729788 , 0.48741403, 0.3858711 , 0.65917736, 0.38613766,\n",
       "       0.43865415, 0.8870549 , 0.9443282 , 0.88976693, 0.42277864,\n",
       "       0.5158559 , 0.46581185, 0.46592662, 0.42864525, 0.4127088 ,\n",
       "       0.35591227, 0.38705894, 0.3507833 , 0.39953735, 0.820135  ,\n",
       "       0.41156927, 0.45887703, 0.6077236 , 0.4030287 , 0.3624907 ,\n",
       "       0.64357275, 0.47348893, 0.40697816, 0.38519642, 0.4671661 ,\n",
       "       0.508009  , 0.5488398 , 0.46940106, 0.3810812 , 0.55804515,\n",
       "       0.4285453 , 0.31345358, 0.46241042, 0.4692177 , 0.41936842,\n",
       "       0.53248256, 0.41279435, 0.3765208 , 0.48355323, 0.35148123,\n",
       "       0.37141368, 0.43885556, 0.5189934 , 0.45058924, 0.41268662,\n",
       "       0.57056963, 0.37420473, 0.45600903, 0.49669603, 0.41327703,\n",
       "       0.3378682 , 0.33618075, 0.42518148, 0.38553876, 0.894799  ,\n",
       "       0.40806195, 0.4116503 , 0.57182896, 0.45399436, 0.33000335,\n",
       "       0.46472543, 0.4787259 , 0.2822306 , 0.7817876 , 0.44744515,\n",
       "       0.6965296 , 0.3923737 , 0.9568899 , 0.37508473, 0.9590081 ,\n",
       "       0.34403402, 0.87893826, 0.41496584, 0.6878757 , 0.44627595,\n",
       "       0.33830488, 0.5701718 , 0.3880132 , 0.53347105, 0.44550222,\n",
       "       0.6347835 , 0.64699405, 0.5250813 , 0.35135758, 0.49356133,\n",
       "       0.46081945, 0.5227143 , 0.47907653, 0.38373807, 0.4195357 ,\n",
       "       0.6149414 , 0.42900005, 0.42952642, 0.48409393, 0.38739502,\n",
       "       0.6232874 , 0.81104034, 0.5065989 , 0.32502216, 0.56388193,\n",
       "       0.3980884 , 0.4458021 , 0.53020024, 0.67490333, 0.61028206,\n",
       "       0.6768569 , 0.38740438, 0.32435405, 0.4458765 , 0.57174724,\n",
       "       0.47508627, 0.41683063, 0.4481543 , 0.5432769 , 0.58076346,\n",
       "       0.954416  , 0.4967218 , 0.4219409 , 0.40501153, 0.38717666,\n",
       "       0.5120901 , 0.9560847 , 0.4670024 , 0.6649046 , 0.43147406,\n",
       "       0.691861  , 0.5202962 , 0.52705294, 0.4189447 , 0.8176631 ,\n",
       "       0.49203718, 0.55274355, 0.64570236, 0.44122747, 0.39215258,\n",
       "       0.3085134 , 0.4813731 , 0.4054033 , 0.35518542, 0.515811  ,\n",
       "       0.43992215, 0.47468346, 0.41035122, 0.38946474, 0.8070175 ,\n",
       "       0.6959542 , 0.41782203, 0.5707123 , 0.6131496 , 0.4075181 ,\n",
       "       0.4219429 , 0.6762188 , 0.4543211 , 0.38695046, 0.41741794,\n",
       "       0.41586843, 0.40116572, 0.2947963 , 0.42129532, 0.52928793,\n",
       "       0.42643616, 0.48243818, 0.4058723 , 0.46943694, 0.4830291 ,\n",
       "       0.58440024, 0.3847304 , 0.39494455, 0.46521953, 0.4206346 ,\n",
       "       0.48651397, 0.5228118 , 0.36378706, 0.59892595, 0.41438875,\n",
       "       0.52100265, 0.40644002, 0.3860729 , 0.46131966, 0.38093755,\n",
       "       0.72130376, 0.49044806, 0.5289106 , 0.568161  , 0.6014038 ,\n",
       "       0.63183796], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \"scratch\" with Bert Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_unstemmed,\n",
    "                                                    y_encoded,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready output data for the model\n",
    "max_length=250\n",
    "\n",
    "x_train_toke = tokenizer(text=x_train.to_list(),\n",
    "                         add_special_tokens=True,\n",
    "                         max_length=max_length,\n",
    "                         truncation=True,\n",
    "                         padding=True, \n",
    "                         return_tensors='tf',\n",
    "                         return_token_type_ids=False,\n",
    "                         return_attention_mask=False,\n",
    "                         verbose=True)\n",
    "\n",
    "x_test_toke = tokenizer(text=x_test.to_list(),\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='tf',\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=False,\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xInput = Input(shape=(max_length))\n",
    "x_ = Dense(700, activation='relu')(xInput)\n",
    "x_ = Dense(700, activation='relu')(x_)\n",
    "output = Dense(len(y_encoded[0]), activation='sigmoid')(x_)\n",
    "\n",
    "model_toke = Model(inputs=xInput, outputs=output, name='Toke')\n",
    "\n",
    "# Compile the model\n",
    "model_toke.compile(loss=CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "                   optimizer=Adam(learning_rate=0.00000005),\n",
    "                   metrics=[CategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,\n",
    "                                         mode='max',\n",
    "                                         restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Toke\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 250)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 700)               175700    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700)               490700    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 206)               144406    \n",
      "=================================================================\n",
      "Total params: 810,806\n",
      "Trainable params: 810,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_toke.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 10815.0297 - accuracy: 0.0023 - val_loss: 10445.2637 - val_accuracy: 0.0027\n",
      "Epoch 2/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 10349.4805 - accuracy: 0.0028 - val_loss: 10029.6533 - val_accuracy: 0.0027\n",
      "Epoch 3/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9865.9587 - accuracy: 0.0033 - val_loss: 9662.8994 - val_accuracy: 0.0032\n",
      "Epoch 4/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9557.3737 - accuracy: 0.0034 - val_loss: 9331.8984 - val_accuracy: 0.0032\n",
      "Epoch 5/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 9129.3859 - accuracy: 0.0040 - val_loss: 9030.4053 - val_accuracy: 0.0035\n",
      "Epoch 6/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8885.0665 - accuracy: 0.0037 - val_loss: 8752.3252 - val_accuracy: 0.0032\n",
      "Epoch 7/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8666.4084 - accuracy: 0.0048 - val_loss: 8493.9473 - val_accuracy: 0.0029\n",
      "Epoch 8/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8375.7637 - accuracy: 0.0052 - val_loss: 8252.1250 - val_accuracy: 0.0029\n",
      "Epoch 9/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8163.2896 - accuracy: 0.0057 - val_loss: 8025.8628 - val_accuracy: 0.0029\n",
      "Epoch 10/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 8007.8396 - accuracy: 0.0055 - val_loss: 7813.4385 - val_accuracy: 0.0027\n",
      "Epoch 11/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 7676.6330 - accuracy: 0.0056 - val_loss: 7613.3716 - val_accuracy: 0.0029\n",
      "Epoch 12/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 7501.3251 - accuracy: 0.0066 - val_loss: 7426.3286 - val_accuracy: 0.0035\n",
      "Epoch 13/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 7314.4328 - accuracy: 0.0070 - val_loss: 7251.9121 - val_accuracy: 0.0050\n",
      "Epoch 14/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 7149.4009 - accuracy: 0.0079 - val_loss: 7089.4067 - val_accuracy: 0.0069\n",
      "Epoch 15/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6981.9439 - accuracy: 0.0099 - val_loss: 6938.9463 - val_accuracy: 0.0093\n",
      "Epoch 16/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6882.9546 - accuracy: 0.0099 - val_loss: 6801.4346 - val_accuracy: 0.0096\n",
      "Epoch 17/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6761.0634 - accuracy: 0.0109 - val_loss: 6676.3359 - val_accuracy: 0.0109\n",
      "Epoch 18/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6620.8259 - accuracy: 0.0111 - val_loss: 6562.8760 - val_accuracy: 0.0125\n",
      "Epoch 19/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6497.6223 - accuracy: 0.0134 - val_loss: 6459.6025 - val_accuracy: 0.0143\n",
      "Epoch 20/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6428.6348 - accuracy: 0.0157 - val_loss: 6365.2617 - val_accuracy: 0.0151\n",
      "Epoch 21/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6309.5498 - accuracy: 0.0163 - val_loss: 6278.7212 - val_accuracy: 0.0162\n",
      "Epoch 22/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6187.7078 - accuracy: 0.0172 - val_loss: 6198.7515 - val_accuracy: 0.0186\n",
      "Epoch 23/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6115.7256 - accuracy: 0.0199 - val_loss: 6124.8916 - val_accuracy: 0.0191\n",
      "Epoch 24/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6060.7424 - accuracy: 0.0203 - val_loss: 6056.0186 - val_accuracy: 0.0205\n",
      "Epoch 25/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 6014.1831 - accuracy: 0.0231 - val_loss: 5991.6514 - val_accuracy: 0.0220\n",
      "Epoch 26/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5964.9155 - accuracy: 0.0241 - val_loss: 5931.4375 - val_accuracy: 0.0236\n",
      "Epoch 27/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5897.7011 - accuracy: 0.0262 - val_loss: 5874.9082 - val_accuracy: 0.0252\n",
      "Epoch 28/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5790.2275 - accuracy: 0.0270 - val_loss: 5822.1035 - val_accuracy: 0.0271\n",
      "Epoch 29/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5743.0331 - accuracy: 0.0278 - val_loss: 5772.2480 - val_accuracy: 0.0284\n",
      "Epoch 30/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5686.3337 - accuracy: 0.0302 - val_loss: 5725.2803 - val_accuracy: 0.0290\n",
      "Epoch 31/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5669.0333 - accuracy: 0.0300 - val_loss: 5680.5938 - val_accuracy: 0.0295\n",
      "Epoch 32/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5626.8386 - accuracy: 0.0308 - val_loss: 5638.5083 - val_accuracy: 0.0303\n",
      "Epoch 33/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5563.4242 - accuracy: 0.0314 - val_loss: 5599.0298 - val_accuracy: 0.0300\n",
      "Epoch 34/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5504.6793 - accuracy: 0.0326 - val_loss: 5561.3833 - val_accuracy: 0.0308\n",
      "Epoch 35/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5475.2710 - accuracy: 0.0329 - val_loss: 5525.5312 - val_accuracy: 0.0313\n",
      "Epoch 36/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5513.2460 - accuracy: 0.0327 - val_loss: 5491.7256 - val_accuracy: 0.0324\n",
      "Epoch 37/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5446.4941 - accuracy: 0.0344 - val_loss: 5459.9604 - val_accuracy: 0.0332\n",
      "Epoch 38/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5424.1866 - accuracy: 0.0349 - val_loss: 5430.0967 - val_accuracy: 0.0340\n",
      "Epoch 39/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5359.4361 - accuracy: 0.0341 - val_loss: 5402.4087 - val_accuracy: 0.0356\n",
      "Epoch 40/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5273.7713 - accuracy: 0.0361 - val_loss: 5375.8511 - val_accuracy: 0.0356\n",
      "Epoch 41/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5287.7540 - accuracy: 0.0363 - val_loss: 5350.6499 - val_accuracy: 0.0367\n",
      "Epoch 42/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5277.3042 - accuracy: 0.0375 - val_loss: 5326.7500 - val_accuracy: 0.0369\n",
      "Epoch 43/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5252.0030 - accuracy: 0.0360 - val_loss: 5304.2393 - val_accuracy: 0.0369\n",
      "Epoch 44/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5254.2133 - accuracy: 0.0364 - val_loss: 5283.2368 - val_accuracy: 0.0377\n",
      "Epoch 45/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5233.2391 - accuracy: 0.0380 - val_loss: 5263.5200 - val_accuracy: 0.0380\n",
      "Epoch 46/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5215.4066 - accuracy: 0.0385 - val_loss: 5244.6772 - val_accuracy: 0.0380\n",
      "Epoch 47/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5173.0602 - accuracy: 0.0392 - val_loss: 5226.7759 - val_accuracy: 0.0369\n",
      "Epoch 48/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5177.4994 - accuracy: 0.0368 - val_loss: 5209.9180 - val_accuracy: 0.0369\n",
      "Epoch 49/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5140.8063 - accuracy: 0.0372 - val_loss: 5193.9419 - val_accuracy: 0.0375\n",
      "Epoch 50/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5084.9083 - accuracy: 0.0400 - val_loss: 5178.8706 - val_accuracy: 0.0375\n",
      "Epoch 51/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5140.2478 - accuracy: 0.0402 - val_loss: 5164.3008 - val_accuracy: 0.0369\n",
      "Epoch 52/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5104.8955 - accuracy: 0.0406 - val_loss: 5150.3501 - val_accuracy: 0.0375\n",
      "Epoch 53/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5107.3591 - accuracy: 0.0415 - val_loss: 5136.9644 - val_accuracy: 0.0367\n",
      "Epoch 54/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5086.0698 - accuracy: 0.0401 - val_loss: 5124.1089 - val_accuracy: 0.0361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "1059/1059 [==============================] - 2s 2ms/step - loss: 5032.1105 - accuracy: 0.0410 - val_loss: 5111.3574 - val_accuracy: 0.0361\n",
      "INFO:tensorflow:Assets written to: /home/mlmaster/Code/Ing_ml_P7/ModelTokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "# Load the model, if does not exist then train one\n",
    "if globalStrategy == 'retrain' or globalStrategy == 'retrainToke':\n",
    "    epochs = epochs\n",
    "    batch_size=batch_size\n",
    "    history = model_toke.fit(x_train_toke[\"input_ids\"], y_train,\n",
    "                             epochs=epochs,\n",
    "                             validation_split=0.1,\n",
    "                             callbacks=[callback],\n",
    "                             verbose=1)\n",
    "\n",
    "    model_toke.save('/home/mlmaster/Code/Ing_ml_P7/ModelTokenizer/')\n",
    "    model_toke = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/ModelTokenizer/')\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        model_toke = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/ModelTokenizer/')\n",
    "    except OSError:\n",
    "        epochs = epochs\n",
    "        batch_size = batch_size\n",
    "        history = model_toke.fit(x_train_toke[\"input_ids\"], y_train,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_split=0.1,\n",
    "                                 callbacks=[callback],\n",
    "                                 verbose=1)\n",
    "\n",
    "        model_toke.save('/home/mlmaster/Code/Ing_ml_P7/ModelTokenizer/')\n",
    "        model_toke = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/ModelTokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 0s 1ms/step - loss: 5282.8545 - accuracy: 0.0369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5282.8544921875, 0.03687174618244171]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_toke.evaluate(x_test_toke[\"input_ids\"], y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>Jaccard_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.024484</td>\n",
       "      <td>0.012394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  F1_micro  Jaccard_micro\n",
       "68       0.98  0.024484       0.012394"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model_toke.predict(x_test_toke[\"input_ids\"])\n",
    "# predict_ = np.where(predict > 0.5, 1, 0)\n",
    "scoring(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 8.2217391e-21, 1.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.3500168e-12, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 8.4029880e-27, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 9.9999857e-01, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.8422307e-01,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.2174887e-27,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       2.2697424e-33, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read msg files read msg files need read outlook msg file net without com api outlook cos installed machines app run free 3rd party libraries want extract cc fields sent receive date fields would good also stored msg files\n",
      "['read', 'ms', '##g', 'files', 'read', 'ms', '##g', 'files', 'need', 'read', 'outlook', 'ms', '##g', 'file', 'net', 'without', 'com', 'api', 'outlook', 'co', '##s', 'installed', 'machines', 'app', 'run', 'free', '3rd', 'party', 'libraries', 'want', 'extract', 'cc', 'fields', 'sent', 'receive', 'date', 'fields', 'would', 'good', 'also', 'stored', 'ms', '##g', 'files']\n",
      "tf.Tensor(\n",
      "[  101  8021  3787 10629  2028  2028 18750  8021  3787 10629  2028  2028\n",
      " 18750  2699 20811 10629  9896 18750  3431  2191  2215  6140  3787 10629\n",
      "  2028  2028  2878 10629  2393  2172 12315  4283  1038  2184  1038  2184\n",
      "  1038  2184  2321  2531  2487  1038  6694  6694  6140  4130 13366 10629\n",
      "  2707  2203  4130  4130  4130  2707  2707  2203  2709  4130 10629  2707\n",
      "  2709 10425 13045 10629  2707 13045  4130 10629 13045  2203  4130 10425\n",
      " 10439 10497  2709 10425   102     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0], shape=(250,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[15])\n",
    "print(tokenizer.tokenize(x_train[15]))\n",
    "print(x_train_toke[\"input_ids\"][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laravel use statement non name cache effect laravel use statement non name cache effect laravel 5 memcached try use cache routes php routes php line 3 use statement non name cache php cache\n",
      "['lara', '##vel', 'use', 'statement', 'non', 'name', 'cache', 'effect', 'lara', '##vel', 'use', 'statement', 'non', 'name', 'cache', 'effect', 'lara', '##vel', '5', 'me', '##mc', '##ache', '##d', 'try', 'use', 'cache', 'routes', 'php', 'routes', 'php', 'line', '3', 'use', 'statement', 'non', 'name', 'cache', 'php', 'cache']\n",
      "tf.Tensor(\n",
      "[  101  6164 21025  2102  8833 21025  2102  4487  4246  6164 21025  2102\n",
      "  8833 21025  2102  4487  4246  2667  4553 21025  2102  2393 21025  2102\n",
      "  2028  2518  7188  2224 21025  2102  8833 21025  2102  4487  4246  3275\n",
      "  2279  8087  2203  2773  2828 10954  2203  5494  2783 24234  3332  2330\n",
      "  2178  2828  2279  3094  2215  2224   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0], shape=(250,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[2])\n",
    "print(tokenizer.tokenize(x_train[2]))\n",
    "print(x_train_toke[\"input_ids\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model with Bert Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the tokenizer we had before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Name of the BERT model to use\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "# Load the Transformers BERT model\n",
    "transformer_model = TFBertModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7f943b64a880>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load bert layers\n",
    "bert = transformer_model.layers[0]\n",
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lock layers of bert so they don't get train again\n",
    "bert.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser l'api functional\n",
    "# Build the model\n",
    "inputs = Input(shape=(max_length,), dtype='int32')\n",
    "bert_model = bert(inputs)[1]\n",
    "bert_model = Dropout(0.2)(bert_model, training=False)\n",
    "bert_model = Dense(len(y_encoded[0]), activation='sigmoid', name='categ')(bert_model)\n",
    "\n",
    "# Combine\n",
    "myBert = Model(inputs=inputs, \n",
    "               outputs=bert_model, \n",
    "               name='BERT_MultiLabel_MultiClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,\n",
    "                                         mode='max',\n",
    "                                         restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 250)]             0         \n",
      "_________________________________________________________________\n",
      "bert (TFBertMainLayer)       TFBaseModelOutputWithPool 109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "categ (Dense)                (None, 206)               158414    \n",
      "=================================================================\n",
      "Total params: 109,640,654\n",
      "Trainable params: 158,414\n",
      "Non-trainable params: 109,482,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myBert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an optimizer\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-05,\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss = {'categ': CategoricalCrossentropy(from_logits=True)}\n",
    "metric = {'categ': CategoricalAccuracy('accuracy')}\n",
    "\n",
    "# Compile the model\n",
    "myBert.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss, \n",
    "    metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "530/530 [==============================] - 793s 1s/step - loss: 9.4803 - accuracy: 0.0314 - val_loss: 8.6683 - val_accuracy: 0.0454\n",
      "Epoch 2/200\n",
      "530/530 [==============================] - 792s 1s/step - loss: 8.4915 - accuracy: 0.0468 - val_loss: 8.5314 - val_accuracy: 0.0481\n",
      "Epoch 3/200\n",
      "530/530 [==============================] - 792s 1s/step - loss: 8.3208 - accuracy: 0.0485 - val_loss: 8.5022 - val_accuracy: 0.0494\n",
      "Epoch 4/200\n",
      "530/530 [==============================] - 793s 1s/step - loss: 8.3743 - accuracy: 0.0542 - val_loss: 8.4980 - val_accuracy: 0.0486\n",
      "Epoch 5/200\n",
      "530/530 [==============================] - 795s 1s/step - loss: 8.3246 - accuracy: 0.0539 - val_loss: 8.5019 - val_accuracy: 0.0494\n",
      "Epoch 6/200\n",
      "530/530 [==============================] - 795s 1s/step - loss: 8.3057 - accuracy: 0.0534 - val_loss: 8.5085 - val_accuracy: 0.0491\n",
      "Epoch 7/200\n",
      "530/530 [==============================] - 795s 1s/step - loss: 8.3596 - accuracy: 0.0563 - val_loss: 8.5159 - val_accuracy: 0.0494\n",
      "Epoch 8/200\n",
      "530/530 [==============================] - 795s 1s/step - loss: 8.3433 - accuracy: 0.0593 - val_loss: 8.5235 - val_accuracy: 0.0494\n",
      "Epoch 9/200\n",
      "530/530 [==============================] - 795s 1s/step - loss: 8.3657 - accuracy: 0.0587 - val_loss: 8.5312 - val_accuracy: 0.0494\n",
      "Epoch 10/200\n",
      "530/530 [==============================] - 795s 1s/step - loss: 8.4039 - accuracy: 0.0573 - val_loss: 8.5384 - val_accuracy: 0.0497\n",
      "Epoch 11/200\n",
      "530/530 [==============================] - 798s 2s/step - loss: 8.3946 - accuracy: 0.0574 - val_loss: 8.5453 - val_accuracy: 0.0499\n",
      "Epoch 12/200\n",
      "530/530 [==============================] - 803s 2s/step - loss: 8.4166 - accuracy: 0.0601 - val_loss: 8.5518 - val_accuracy: 0.0499\n",
      "Epoch 13/200\n",
      "530/530 [==============================] - 803s 2s/step - loss: 8.4002 - accuracy: 0.0612 - val_loss: 8.5579 - val_accuracy: 0.0494\n",
      "Epoch 14/200\n",
      "530/530 [==============================] - 805s 2s/step - loss: 8.4205 - accuracy: 0.0586 - val_loss: 8.5638 - val_accuracy: 0.0497\n",
      "Epoch 15/200\n",
      "530/530 [==============================] - 806s 2s/step - loss: 8.4052 - accuracy: 0.0613 - val_loss: 8.5692 - val_accuracy: 0.0499\n",
      "Epoch 16/200\n",
      "530/530 [==============================] - 806s 2s/step - loss: 8.4551 - accuracy: 0.0577 - val_loss: 8.5744 - val_accuracy: 0.0505\n",
      "Epoch 17/200\n",
      "530/530 [==============================] - 807s 2s/step - loss: 8.4851 - accuracy: 0.0605 - val_loss: 8.5792 - val_accuracy: 0.0505\n",
      "Epoch 18/200\n",
      "530/530 [==============================] - 808s 2s/step - loss: 8.4316 - accuracy: 0.0603 - val_loss: 8.5839 - val_accuracy: 0.0505\n",
      "Epoch 19/200\n",
      "530/530 [==============================] - 808s 2s/step - loss: 8.4360 - accuracy: 0.0630 - val_loss: 8.5881 - val_accuracy: 0.0507\n",
      "Epoch 20/200\n",
      "530/530 [==============================] - 805s 2s/step - loss: 8.4724 - accuracy: 0.0595 - val_loss: 8.5922 - val_accuracy: 0.0513\n",
      "Epoch 21/200\n",
      "530/530 [==============================] - 806s 2s/step - loss: 8.4342 - accuracy: 0.0596 - val_loss: 8.5960 - val_accuracy: 0.0510\n",
      "Epoch 22/200\n",
      "530/530 [==============================] - 807s 2s/step - loss: 8.4455 - accuracy: 0.0610 - val_loss: 8.5996 - val_accuracy: 0.0513\n",
      "Epoch 23/200\n",
      "530/530 [==============================] - 806s 2s/step - loss: 8.4469 - accuracy: 0.0617 - val_loss: 8.6030 - val_accuracy: 0.0515\n",
      "Epoch 24/200\n",
      "530/530 [==============================] - 806s 2s/step - loss: 8.4708 - accuracy: 0.0625 - val_loss: 8.6063 - val_accuracy: 0.0518\n",
      "Epoch 25/200\n",
      "530/530 [==============================] - 807s 2s/step - loss: 8.5180 - accuracy: 0.0608 - val_loss: 8.6095 - val_accuracy: 0.0515\n",
      "Epoch 26/200\n",
      "530/530 [==============================] - 807s 2s/step - loss: 8.5128 - accuracy: 0.0604 - val_loss: 8.6125 - val_accuracy: 0.0521\n",
      "Epoch 27/200\n",
      "530/530 [==============================] - 803s 2s/step - loss: 8.4803 - accuracy: 0.0613 - val_loss: 8.6154 - val_accuracy: 0.0515\n",
      "Epoch 28/200\n",
      "530/530 [==============================] - 804s 2s/step - loss: 8.4636 - accuracy: 0.0612 - val_loss: 8.6182 - val_accuracy: 0.0515\n",
      "Epoch 29/200\n",
      "530/530 [==============================] - 804s 2s/step - loss: 8.4808 - accuracy: 0.0615 - val_loss: 8.6208 - val_accuracy: 0.0518\n",
      "Epoch 30/200\n",
      "530/530 [==============================] - 805s 2s/step - loss: 8.4921 - accuracy: 0.0665 - val_loss: 8.6234 - val_accuracy: 0.0518\n",
      "Epoch 31/200\n",
      "530/530 [==============================] - 806s 2s/step - loss: 8.5001 - accuracy: 0.0628 - val_loss: 8.6259 - val_accuracy: 0.0515\n",
      "Epoch 32/200\n",
      "530/530 [==============================] - 807s 2s/step - loss: 8.5273 - accuracy: 0.0629 - val_loss: 8.6283 - val_accuracy: 0.0518\n",
      "Epoch 33/200\n",
      "530/530 [==============================] - 808s 2s/step - loss: 8.4764 - accuracy: 0.0634 - val_loss: 8.6306 - val_accuracy: 0.0526\n",
      "Epoch 34/200\n",
      "530/530 [==============================] - 809s 2s/step - loss: 8.4777 - accuracy: 0.0640 - val_loss: 8.6329 - val_accuracy: 0.0521\n",
      "Epoch 35/200\n",
      "530/530 [==============================] - 808s 2s/step - loss: 8.4616 - accuracy: 0.0634 - val_loss: 8.6350 - val_accuracy: 0.0518\n",
      "Epoch 36/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.5215 - accuracy: 0.0616 - val_loss: 8.6371 - val_accuracy: 0.0523\n",
      "Epoch 37/200\n",
      "530/530 [==============================] - 802s 2s/step - loss: 8.5232 - accuracy: 0.0638 - val_loss: 8.6392 - val_accuracy: 0.0526\n",
      "Epoch 38/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.5602 - accuracy: 0.0623 - val_loss: 8.6412 - val_accuracy: 0.0523\n",
      "Epoch 39/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4951 - accuracy: 0.0636 - val_loss: 8.6430 - val_accuracy: 0.0526\n",
      "Epoch 40/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4807 - accuracy: 0.0652 - val_loss: 8.6449 - val_accuracy: 0.0529\n",
      "Epoch 41/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4839 - accuracy: 0.0612 - val_loss: 8.6467 - val_accuracy: 0.0526\n",
      "Epoch 42/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4921 - accuracy: 0.0623 - val_loss: 8.6484 - val_accuracy: 0.0526\n",
      "Epoch 43/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4947 - accuracy: 0.0622 - val_loss: 8.6501 - val_accuracy: 0.0526\n",
      "Epoch 44/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.5184 - accuracy: 0.0610 - val_loss: 8.6518 - val_accuracy: 0.0518\n",
      "Epoch 45/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4940 - accuracy: 0.0613 - val_loss: 8.6534 - val_accuracy: 0.0518\n",
      "Epoch 46/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4885 - accuracy: 0.0624 - val_loss: 8.6550 - val_accuracy: 0.0518\n",
      "Epoch 47/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4932 - accuracy: 0.0647 - val_loss: 8.6565 - val_accuracy: 0.0521\n",
      "Epoch 48/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.5511 - accuracy: 0.0637 - val_loss: 8.6580 - val_accuracy: 0.0523\n",
      "Epoch 49/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4883 - accuracy: 0.0676 - val_loss: 8.6594 - val_accuracy: 0.0518\n",
      "Epoch 50/200\n",
      "530/530 [==============================] - 801s 2s/step - loss: 8.4999 - accuracy: 0.0597 - val_loss: 8.6608 - val_accuracy: 0.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/mlmaster/Code/Ing_ml_P7/myBert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/mlmaster/Code/Ing_ml_P7/myBert/assets\n"
     ]
    }
   ],
   "source": [
    "if globalStrategy == 'retrain' or globalStrategy == 'retrainBert':\n",
    "    epochs = epochs\n",
    "    batch_size=batch_size\n",
    "    history = myBert.fit(x=x_train_toke[\"input_ids\"],\n",
    "                         y=y_train,validation_split=0.1,\n",
    "                         batch_size=batch_size,\n",
    "                         callbacks=[callback],\n",
    "                         epochs=epochs)\n",
    "\n",
    "    myBert.save('/home/mlmaster/Code/Ing_ml_P7/myBert/')\n",
    "    myBert = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/myBert/')\n",
    "else:\n",
    "    try:\n",
    "        myBert = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/myBert/')\n",
    "    except OSError:\n",
    "        epochs = epochs\n",
    "        batch_size=batch_size\n",
    "        history = myBert.fit(x=x_train_toke[\"input_ids\"],\n",
    "                             y=y_train,validation_split=0.1,\n",
    "                             batch_size=batch_size,\n",
    "                             callbacks=[callback],\n",
    "                             epochs=epochs)\n",
    "\n",
    "        myBert.save('/home/mlmaster/Code/Ing_ml_P7/myBert/')\n",
    "        myBert = tf.keras.models.load_model('/home/mlmaster/Code/Ing_ml_P7/myBert/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 184s 621ms/step - loss: 8.5197 - accuracy: 0.0600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.519692420959473, 0.060036126524209976]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evaluation\n",
    "model_eval = myBert.evaluate(x=x_test_toke[\"input_ids\"],\n",
    "                             y=y_test)\n",
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = myBert.predict(x_test_toke[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>Jaccard_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.116102</td>\n",
       "      <td>0.061628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  F1_micro  Jaccard_micro\n",
       "68       0.98  0.116102       0.061628"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict_ = np.where(predict > 0.95, 1, 0) # Threshold is very high but necessary as model outputs > 0.5 for all\n",
    "scoring(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.978031  , 0.65992993, 0.8706724 , 0.8981209 , 0.97752416,\n",
       "       0.47003224, 0.47338086, 0.69648725, 0.8028426 , 0.7732695 ,\n",
       "       0.87529725, 0.7327794 , 0.56030226, 0.97619015, 0.95441955,\n",
       "       0.9061052 , 0.6354614 , 0.60939306, 0.9415106 , 0.6028231 ,\n",
       "       0.469499  , 0.9836626 , 0.991642  , 0.9880163 , 0.75687283,\n",
       "       0.838966  , 0.57190037, 0.63525856, 0.6838393 , 0.75669706,\n",
       "       0.60401016, 0.5906911 , 0.5866819 , 0.59073126, 0.98210883,\n",
       "       0.64273185, 0.61359364, 0.9288827 , 0.6943421 , 0.82769406,\n",
       "       0.8117501 , 0.800745  , 0.6270091 , 0.5690289 , 0.7658088 ,\n",
       "       0.73644376, 0.65136987, 0.8059165 , 0.6456533 , 0.7654775 ,\n",
       "       0.702444  , 0.60459274, 0.7619956 , 0.5977242 , 0.49849996,\n",
       "       0.81299084, 0.53426045, 0.6918267 , 0.83873296, 0.6689777 ,\n",
       "       0.4455477 , 0.589231  , 0.79841715, 0.75317484, 0.5828179 ,\n",
       "       0.8485511 , 0.58634484, 0.79908854, 0.86458164, 0.6054993 ,\n",
       "       0.55012584, 0.6212171 , 0.55838066, 0.6405807 , 0.99234253,\n",
       "       0.698122  , 0.58635134, 0.81734526, 0.8106289 , 0.53809905,\n",
       "       0.61643827, 0.49684677, 0.47888327, 0.9706002 , 0.64692026,\n",
       "       0.9436357 , 0.6803354 , 0.9930322 , 0.5511515 , 0.9926402 ,\n",
       "       0.52230173, 0.98653203, 0.5875731 , 0.9420955 , 0.68107736,\n",
       "       0.7082114 , 0.69319636, 0.71589017, 0.79971737, 0.8397011 ,\n",
       "       0.95562226, 0.92148125, 0.844482  , 0.4639037 , 0.81245583,\n",
       "       0.74551183, 0.790565  , 0.5896057 , 0.5574225 , 0.5437531 ,\n",
       "       0.588361  , 0.52647424, 0.5927923 , 0.76787573, 0.59717304,\n",
       "       0.8292475 , 0.98037875, 0.6520604 , 0.55682325, 0.90008754,\n",
       "       0.47382995, 0.6946587 , 0.808963  , 0.9196738 , 0.8875362 ,\n",
       "       0.92633665, 0.656101  , 0.5716441 , 0.769844  , 0.8499767 ,\n",
       "       0.6321471 , 0.5489845 , 0.70782393, 0.8701485 , 0.79867035,\n",
       "       0.9938626 , 0.834215  , 0.61610717, 0.6159296 , 0.5203542 ,\n",
       "       0.80101013, 0.99205935, 0.8452072 , 0.9555893 , 0.6800903 ,\n",
       "       0.951336  , 0.76295197, 0.834245  , 0.49169225, 0.9639083 ,\n",
       "       0.7414295 , 0.9279519 , 0.8826776 , 0.7987871 , 0.51573163,\n",
       "       0.56097215, 0.71634334, 0.5920591 , 0.49357387, 0.9119909 ,\n",
       "       0.49879852, 0.7970022 , 0.5385717 , 0.72414386, 0.96566075,\n",
       "       0.92636365, 0.6031954 , 0.51284385, 0.948649  , 0.49993765,\n",
       "       0.65051425, 0.8033438 , 0.77924657, 0.73632675, 0.71649045,\n",
       "       0.70831734, 0.6790816 , 0.6127146 , 0.58228487, 0.7402128 ,\n",
       "       0.6196464 , 0.7826209 , 0.5915263 , 0.764236  , 0.6847346 ,\n",
       "       0.80295616, 0.6925154 , 0.7618686 , 0.79518324, 0.73720634,\n",
       "       0.6639903 , 0.67734563, 0.5223001 , 0.8821508 , 0.58268094,\n",
       "       0.7250036 , 0.6644021 , 0.66320527, 0.5194632 , 0.48406374,\n",
       "       0.9236977 , 0.82145804, 0.81242025, 0.83250546, 0.768035  ,\n",
       "       0.880572  ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
